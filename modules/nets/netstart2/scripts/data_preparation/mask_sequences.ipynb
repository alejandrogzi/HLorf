{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the compressed CSV files for each partition into dfs\n",
    "data_partition_1 = pd.read_csv(\n",
    "    \"../../data/data_model/datasets/data_partition_1.csv.gz\",\n",
    "    dtype={\"annotation_source\": \"str\"},\n",
    "    compression=\"gzip\",\n",
    ")\n",
    "data_partition_2 = pd.read_csv(\n",
    "    \"../../data/data_model/datasets/data_partition_2.csv.gz\",\n",
    "    dtype={\"annotation_source\": \"str\"},\n",
    "    compression=\"gzip\",\n",
    ")\n",
    "data_partition_3 = pd.read_csv(\n",
    "    \"../../data/data_model/datasets/data_partition_3.csv.gz\",\n",
    "    dtype={\"annotation_source\": \"str\"},\n",
    "    compression=\"gzip\",\n",
    ")\n",
    "data_partition_4 = pd.read_csv(\n",
    "    \"../../data/data_model/datasets/data_partition_4.csv.gz\",\n",
    "    dtype={\"annotation_source\": \"str\"},\n",
    "    compression=\"gzip\",\n",
    ")\n",
    "data_partition_5 = pd.read_csv(\n",
    "    \"../../data/data_model/datasets/data_partition_5.csv.gz\",\n",
    "    dtype={\"annotation_source\": \"str\"},\n",
    "    compression=\"gzip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all species\n",
    "species_list = [\n",
    "    \"Saccharomyces cerevisiae\",\n",
    "    \"Ustilago maydis\",\n",
    "    \"Schizosaccharomyces pombe\",\n",
    "    \"Aspergillus nidulans\",\n",
    "    \"Cryptococcus neoformans\",\n",
    "    \"Neurospora crassa\",\n",
    "    \"Coprinopsis cinerea\",\n",
    "    \"Rhizophagus irregularis\",\n",
    "    \"Schizophyllum commune\",\n",
    "    \"Plasmodium falciparum\",\n",
    "    \"Entamoeba histolytica\",\n",
    "    \"Dictyostelium discoideum\",\n",
    "    \"Giardia intestinalis\",\n",
    "    \"Trypanosoma brucei\",\n",
    "    \"Leishmania donovani\",\n",
    "    \"Toxoplasma gondii\",\n",
    "    \"Eimeria maxima\",\n",
    "    \"Oryza sativa\",\n",
    "    \"Arabidopsis thaliana\",\n",
    "    \"Selaginella moellendorffii\",\n",
    "    \"Brachypodium distachyon\",\n",
    "    \"Setaria viridis\",\n",
    "    \"Zea mays\",\n",
    "    \"Hordeum vulgare\",\n",
    "    \"Triticum aestivum\",\n",
    "    \"Phoenix dactylifera\",\n",
    "    \"Lotus japonicus\",\n",
    "    \"Medicago truncatula\",\n",
    "    \"Nicotiana tabacum\",\n",
    "    \"Glycine max\",\n",
    "    \"Solanum lycopersicum\",\n",
    "    \"Trichoplax adhaerens\",\n",
    "    \"Tribolium castaneum\",\n",
    "    \"Manduca sexta\",\n",
    "    \"Apis mellifera\",\n",
    "    \"Strongylocentrotus purpuratus\",\n",
    "    \"Daphnia carinata\",\n",
    "    \"Drosophila melanogaster\",\n",
    "    \"Anopheles gambiae\",\n",
    "    \"Caenorhabditis elegans\",\n",
    "    \"Gallus gallus\",\n",
    "    \"Alligator mississippiensis\",\n",
    "    \"Xenopus laevis\",\n",
    "    \"Oreochromis niloticus\",\n",
    "    \"Homo sapiens\",\n",
    "    \"Bos taurus\",\n",
    "    \"Mus musculus\",\n",
    "    \"Ovis aries\",\n",
    "    \"Canis lupus familiaris\",\n",
    "    \"Equus caballus\",\n",
    "    \"Gorilla gorilla\",\n",
    "    \"Pan troglodytes\",\n",
    "    \"Rattus norvegicus\",\n",
    "    \"Oryctolagus cuniculus\",\n",
    "    \"Sus scrofa\",\n",
    "    \"Danio rerio\",\n",
    "    \"Oryzias latipes\",\n",
    "    \"Taeniopygia guttata\",\n",
    "    \"Columba livia\",\n",
    "    \"Anolis carolinensis\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_sequence(row, reference_df):\n",
    "    \"\"\"\n",
    "    Extends the sequence in the given row by finding the matching sequence in the reference dataframe and appending 100 downstream nucleotides.\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A row from the data_partition_masked dataframe containing the sequence to be extended.\n",
    "        reference_df (pd.DataFrame): The reference dataframe containing the full sequences to find the downstream nucleotides.\n",
    "\n",
    "    Returns:\n",
    "        result (str): The extended sequence with 100 downstream nucleotides appended if a match is found, otherwise the original sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    # Find the matching row in reference_df using the seq_number/Seq_number mapping\n",
    "    match = reference_df[reference_df[\"Seq_number\"] == row[\"seq_number\"]]\n",
    "\n",
    "    if not match.empty:\n",
    "        # Get the data_partition_masked sequence and reference_df sequence\n",
    "        data_partition_masked_sequence = row[\"Sequence\"]\n",
    "        reference_sequence = match.iloc[0][\"Sequence\"]\n",
    "\n",
    "        # Find the overlap position\n",
    "        overlap_pos = reference_sequence.find(data_partition_masked_sequence)\n",
    "        if overlap_pos != -1:\n",
    "            # Extract 100 downstream nucleotides\n",
    "            downstream_start = overlap_pos + len(data_partition_masked_sequence)\n",
    "            downstream_end = downstream_start + 100\n",
    "            downstream_seq = reference_sequence[downstream_start:downstream_end]\n",
    "\n",
    "            result = data_partition_masked_sequence + downstream_seq\n",
    "\n",
    "            # Assertion: Ensure the returned value is a string and includes the original sequence\n",
    "            assert isinstance(result, str), \"Output is not a string.\"\n",
    "            assert data_partition_masked_sequence in result, (\n",
    "                \"Original sequence not present in the output.\"\n",
    "            )\n",
    "            assert len(result) >= len(data_partition_masked_sequence), (\n",
    "                \"Output sequence is shorter than expected.\"\n",
    "            )\n",
    "\n",
    "            return result\n",
    "\n",
    "    # Assertion: If no match is found, ensure the original sequence is returned\n",
    "    assert isinstance(row[\"Sequence\"], str), \"Original sequence is not a string.\"\n",
    "    return row[\"Sequence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_non_TIS_samples(data_partition, number):\n",
    "    \"\"\"\n",
    "    #Mask upstream seqeunce in non-TIS sequences corresponding to X% in TIS-sequences, with length distributions as in TIS sequences.\n",
    "\n",
    "    Args:\n",
    "        data_partition (pd.DataFrame): The dataframe containing the sequences to be masked.\n",
    "        number (int): The partition number for the data_partition dataframe.\n",
    "\n",
    "    Output:\n",
    "        data_partition_masked (pd.DataFrame): The dataframe containing the masked sequences.\n",
    "    \"\"\"\n",
    "    data_partition_masked_full = pd.DataFrame()\n",
    "\n",
    "    masked_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for species in species_list:\n",
    "        # print(species)\n",
    "        data_partition_masked = pd.DataFrame()\n",
    "        # Get TIS data, find fraction of sequences with masked nucleotides and distribution of number of masked nucleotides\n",
    "        data_species = data_partition[data_partition[\"Species\"] == species]\n",
    "\n",
    "        TIS_data = data_species[data_species[\"TIS\"] == 1]\n",
    "        TIS_masked_seqs = TIS_data[TIS_data[\"Sequence\"].str.contains(\"N\")]\n",
    "        TIS_masked_seqs_fracs = TIS_masked_seqs.shape[0] / TIS_data.shape[0]\n",
    "        TIS_masked_seqs = TIS_masked_seqs.copy()  # Ensures itâ€™s a new DataFrame\n",
    "        TIS_masked_seqs[\"N_count\"] = TIS_masked_seqs[\"Sequence\"].str.count(\"N\")\n",
    "        N_distribution = TIS_masked_seqs[\"N_count\"].value_counts().sort_index()\n",
    "\n",
    "        total_count += TIS_data.shape[0]\n",
    "        masked_count += TIS_masked_seqs.shape[0]\n",
    "\n",
    "        # Get non-TIS data\n",
    "        non_TIS_data = data_species[data_species[\"TIS\"] == 0]\n",
    "        non_TIS_samples_to_mask = int(len(non_TIS_data) * TIS_masked_seqs_fracs)\n",
    "        non_TIS_sampled_rows = non_TIS_data.sample(\n",
    "            n=non_TIS_samples_to_mask, random_state=42\n",
    "        ).copy()\n",
    "\n",
    "        assert data_species.shape[0] == non_TIS_data.shape[0] + TIS_data.shape[0]\n",
    "\n",
    "        if non_TIS_sampled_rows.shape[0] != 0:\n",
    "            # Draw the number of \"N\"s for each sampled row based on N_distribution\n",
    "            non_TIS_sampled_rows[\"N_count\"] = np.random.choice(\n",
    "                N_distribution.index,  # Possible counts of \"N\"\n",
    "                size=len(non_TIS_sampled_rows),  # Number of samples to draw\n",
    "                p=N_distribution.values / N_distribution.values.sum(),  # Probabilities\n",
    "            )\n",
    "\n",
    "            # Modify the sequences\n",
    "            def replace_with_N(sequence, n_count):\n",
    "                return \"N\" * n_count + sequence[n_count:]\n",
    "\n",
    "            non_TIS_sampled_rows[\"Sequence\"] = non_TIS_sampled_rows.apply(\n",
    "                lambda row: replace_with_N(row[\"Sequence\"], row[\"N_count\"]), axis=1\n",
    "            )\n",
    "\n",
    "            # Update non-TIS data with modified sequences\n",
    "            non_TIS_data.update(non_TIS_sampled_rows[[\"Sequence\"]])\n",
    "\n",
    "        data_species_masked = pd.concat(\n",
    "            [TIS_data, non_TIS_data], axis=0, ignore_index=True\n",
    "        )\n",
    "\n",
    "        assert data_species_masked.shape[0] == data_species.shape[0]\n",
    "\n",
    "        data_partition_masked = pd.concat(\n",
    "            [data_partition_masked, data_species_masked], axis=0, ignore_index=True\n",
    "        )\n",
    "\n",
    "        # Read original datasets\n",
    "        species_formatted = species.lower().replace(\" \", \"_\")\n",
    "        TIS_seqs_df = pd.read_csv(\n",
    "            f\"../../data/data_model_preparation/datasets/TIS/mRNA_positive_{species_formatted}.csv.gz\",\n",
    "            compression=\"gzip\",\n",
    "        )\n",
    "        non_TIS_seqs_df1 = pd.read_csv(\n",
    "            f\"../../data/data_model_preparation/datasets/non_TIS/mRNA/mRNA_negative_{species_formatted}.csv.gz\",\n",
    "            compression=\"gzip\",\n",
    "        )\n",
    "        non_TIS_seqs_df2 = pd.read_csv(\n",
    "            f\"../../data/data_model_preparation/datasets/non_TIS/intergenic/intergenic_data_{species_formatted}.csv.gz\",\n",
    "            compression=\"gzip\",\n",
    "        )\n",
    "        non_TIS_seqs_df3 = pd.read_csv(\n",
    "            f\"../../data/data_model_preparation/datasets/non_TIS/introns/introns_{species_formatted}.csv.gz\",\n",
    "            compression=\"gzip\",\n",
    "        )\n",
    "\n",
    "        all_seq_data = pd.concat(\n",
    "            [TIS_seqs_df, non_TIS_seqs_df1, non_TIS_seqs_df2, non_TIS_seqs_df3],\n",
    "            axis=0,\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "        # Apply the function to df1\n",
    "        data_partition_masked[\"Sequence\"] = data_partition_masked.apply(\n",
    "            lambda row: extend_sequence(row, all_seq_data), axis=1\n",
    "        )\n",
    "\n",
    "        data_partition_masked_full = pd.concat(\n",
    "            [data_partition_masked_full, data_partition_masked],\n",
    "            axis=0,\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    assert data_partition.shape[0] == data_partition_masked_full.shape[0]\n",
    "\n",
    "    print(\n",
    "        \"Percentage of non-TIS sequences to be padded: \",\n",
    "        round(masked_count / total_count, 4),\n",
    "    )\n",
    "\n",
    "    # Save the DataFrame as a CSV file compressed with gzip\n",
    "    data_partition_masked.to_csv(\n",
    "        f\"../../data/data_model/datasets/data_partition_{str(number)}_masked_extended.csv.gz\",\n",
    "        compression=\"gzip\",\n",
    "        index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58428 232466\n",
      "0.2513\n",
      "58261 232465\n",
      "0.2506\n",
      "58553 232387\n",
      "0.252\n",
      "58339 232416\n",
      "0.251\n",
      "58638 232460\n",
      "0.2522\n"
     ]
    }
   ],
   "source": [
    "mask_non_TIS_samples(data_partition_1, number=1)\n",
    "mask_non_TIS_samples(data_partition_2, number=2)\n",
    "mask_non_TIS_samples(data_partition_3, number=3)\n",
    "mask_non_TIS_samples(data_partition_4, number=4)\n",
    "mask_non_TIS_samples(data_partition_5, number=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
