//! Core module for detecting open reading frames in a query set of reads
//! Alejandro Gonzales-Irribarren, 2025
//!
//! This module contains the main functions for finding open reading frames (ORFs)
//! in a set of aligned reads.
//!
//! In short, every possible open reading frame (ORF) is detected for every
//! read in the query set. For every potential ORF, learning models and databases
//! are used to determine whether the ORF is a true ORF, a false positive.
//! All the data from each reliable ORF is collected and subjected to another
//! learning model trained with true ORFs and false positives. The process is
//! heavily parallelized to offer fast performance on large datasets.

use genepred::{Bed12, GenePred};
use hashbrown::{hash_map::Entry, HashMap};
use memchr::memchr;
use memmap2::Mmap;

use std::collections::HashSet;
use std::fs::File;
use std::io::{BufWriter, Read, Write};
use std::path::{Path, PathBuf};
use std::str::{from_utf8, FromStr};
use std::sync::Arc;

use crate::{cli::BlastArgs, consts::*, utils::*};

/// Run diamont on the a nested orfipy input + translationAi fasta
///
/// # Arguments
///
/// * `args` - The arguments for running the pipeline
///
/// # Returns
///
/// * `None`
///
/// # Example
///
/// ```rust
/// let args = Args::parse();
/// run_blast(args);
/// ```
pub fn run_blast(args: BlastArgs) {
    let dir = args.outdir.join("orf");
    std::fs::create_dir_all(&dir)
        .unwrap_or_else(|e| panic!("ERROR: could not create directory -> {e}!"));

    let fasta = match args.upstream_flank > 0 || args.downstream_flank > 0 {
        true => cut_fasta(
            &args.fasta,
            args.upstream_flank,
            args.downstream_flank,
            &dir,
        ),
        false => args.fasta,
    };

    let pep = run_orfipy(&fasta, &dir);
    let bed = get_bed(&args.bed);

    let orfs = deduplicate(
        &pep,
        true,
        args.orf_min_len,
        args.orf_min_percent,
        "M".as_bytes(),
    );

    let table = get_table(
        orfs,
        bed,
        &dir,
        args.tai,
        args.nmd_distance,
        args.weak_nmd_distance,
        args.atg_distance,
        args.big_exon_dist_to_ej,
    );

    __run_diamond(table, args.database, &dir);
}

/// Cuts the input fasta file by the given flanks
///
/// # Arguments
///
/// * `fasta` - The path to the input fasta file
/// * `upstream_flank` - The number of bases upstream of the ORF to include in the context sequence
/// * `downstream_flank` - The number of bases downstream of the ORF to include in the context sequence
/// * `outdir` - The output directory
///
/// # Returns
///
/// A `PathBuf` containing the path to the cut fasta file
///
/// # Example
///
/// ```rust, ignore
/// let fasta = cut_fasta(&args.fasta, args.upstream_flank, args.downstream_flank, &dir);
/// ```
fn cut_fasta(
    fasta: &PathBuf,
    upstream_flank: usize,
    downstream_flank: usize,
    outdir: &Path,
) -> PathBuf {
    let file = File::open(fasta).unwrap();
    let mmap = unsafe { Mmap::map(&file).unwrap() };
    let data = mmap.as_ref();

    let mut pos = 0;

    let out_fasta = outdir.join("tmp.fa");
    let mut writer = BufWriter::new(
        File::create(&out_fasta).unwrap_or_else(|e| panic!("ERROR: cannot create file -> {e}")),
    );

    while let Some(start) = memchr(b'>', &data[pos..]) {
        let start = pos + start;
        let end = memchr(b'>', &data[start + 1..]).map_or(data.len(), |e| start + 1 + e);
        let entry = &data[start + 1..end];
        let header_end = memchr(b'\n', entry).unwrap();
        let header = from_utf8(&entry[..header_end]).unwrap().trim();
        let record = &entry[header_end + 1..];
        let seq = record
            .iter()
            .filter(|&&b| b != b'\n' && b != b'\r') // Remove newlines and carriage returns
            .cloned()
            .collect::<Vec<u8>>();

        if seq.len() < upstream_flank + downstream_flank {
            panic!(
                "ERROR: sequence is too short for upstream_flank + downstream_flank -> {} + {} = {} > {} for {}",
                upstream_flank,
                downstream_flank,
                upstream_flank + downstream_flank,
                seq.len(),
                header
            );
        }

        let chopped = seq[upstream_flank..seq.len() - downstream_flank].to_vec();

        writer
            .write_all(format!(">{}\n", header).as_bytes())
            .unwrap_or_else(|e| {
                panic!("ERROR: failed to write to file -> {e}");
            });
        writer.write_all(&chopped).unwrap_or_else(|e| {
            panic!("ERROR: failed to write to file -> {e}");
        });
        writer.write_all(b"\n").unwrap_or_else(|e| {
            panic!("ERROR: failed to write to file -> {e}");
        });

        pos = end;
    }

    out_fasta
}

/// Run diamond on the orfipy output
///
/// # Arguments
///
/// * `table` - The orfipy output
/// * `database` - The path to the translationAi fasta
/// * `outdir` - The output directory
///
/// # Returns
///
/// * `None`
///
/// # Example
///
/// ```rust
/// let table = get_table(orfs, bed, &dir, args.tai);
/// __run_diamond(table, DATABASE, &dir);
/// ```
fn __run_diamond(mut table: HashMap<usize, Vec<String>>, database: PathBuf, outdir: &Path) {
    let diamond = outdir.join("orf.diamond");
    let orfs = outdir.join("orf.dedup.pep");

    let cmd = format!(
        "diamond blastp --query {} --db {} --out {} --outfmt 6 qseqid pident qlen slen length qstart qend sstart send evalue --threads 8 --sensitive -e 1e-10",
        orfs.display(),
        database.display(),
        diamond.display()
    );
    println!("INFO: Executing -> {}", cmd);

    std::process::Command::new("bash")
        .arg("-c")
        .arg(cmd)
        .status()
        .unwrap_or_else(|e| panic!("ERROR: failed to execute diamond command -> {e}"));

    let mut seen = HashSet::new();
    let predictions = reader(&diamond)
        .unwrap_or_else(|e| panic!("ERROR: failed to read blast predictions file -> {e}"));

    let mut writer = BufWriter::new(File::create(diamond.with_extension(RESULT)).unwrap_or_else(
        |e| {
            panic!("ERROR: failed to create output file for blast results -> {e}");
        },
    ));

    for line in predictions.lines() {
        let parts: Vec<&str> = line.split('\t').collect();
        let header = parts[0];
        let u_header = header.parse::<usize>().unwrap_or_else(|_| {
            panic!("ERROR: failed to parse header as usize -> {header} in parts: {parts:?}")
        });

        // INFO: only storing first row -> sorted by default
        if seen.contains(header) {
            continue;
        }

        seen.insert(header);

        let lines = table.get(&u_header).unwrap_or_else(|| {
            panic!("ERROR: could not find header {header} in table -> {parts:?}!")
        });

        for line in lines {
            let blast = BlastRecord::from_parts(&parts);
            let output = format!(
                "{}\t{:.2}\t{:e}\t{}\t{}\t{}\n",
                line,
                blast.blast_pid,
                blast.blast_e_value,
                blast.blast_offset,
                blast.blast_alignment_len,
                blast.percent_aligned
            );

            writer.write_all(output.as_bytes()).unwrap_or_else(|e| {
                panic!("ERROR: failed to write to file -> {e}");
            });
        }

        // INFO: table records with no blast hits remain
        table.remove(&u_header);
    }

    for (_, lines) in table.iter() {
        for line in lines {
            let output = format!("{}\t0\t1\t0\t0\t0\n", line);

            writer.write_all(output.as_bytes()).unwrap_or_else(|e| {
                panic!("ERROR: failed to write to file -> {e}");
            });
        }
    }
}

/// Represents a single BLAST alignment record.
#[derive(Debug, Clone, PartialEq)]
pub struct BlastRecord {
    pub blast_pid: f32,           // Percentage of identical matches
    pub blast_e_value: f64,       // E-value of the match
    pub blast_offset: i32,        // Offset in the query sequence where the match starts
    pub blast_alignment_len: u32, // Length of the alignment
    pub percent_aligned: f32,     // Percentage of the query sequence that is aligned
}

impl BlastRecord {
    /// Creates a new `BlastRecord` from a slice of string parts, typically
    /// obtained by splitting a line from a DIAMOND BLAST output.
    ///
    /// The expected format of `parts` corresponds to `diamond blastp --outfmt 6`
    /// output: `qseqid pident qlen slen length qstart qend sstart send evalue`.
    ///
    /// # Arguments
    ///
    /// * `parts` - A slice of string slices, where each element represents a column from the BLAST output.
    ///
    /// # Returns
    ///
    /// A `BlastRecord` instance populated with the parsed data. The `blast_id`
    /// field is initially empty and is expected to be set later.
    ///
    /// # Panics
    ///
    /// This function will panic if:
    /// - `parts` does not contain at least 10 elements.
    /// - Any of the numeric fields (`blast_idx_id`, `blast_pid`, `blast_e_value`,
    ///   `blast_offset` components, `blast_alignment_len`, query length for `percent_aligned`)
    ///   cannot be successfully parsed into their respective types.
    ///
    /// # Example
    ///
    /// Follows this format:
    ///
    /// qseqid pident  qlen    slen   length qstart    qend   sstart   send     evalue
    ///  17      97.2    142     357     141     1       141     217     357     5.09e-93
    ///
    /// ```rust
    /// let parts = ["1", "99.0", "500", "0", "100", "1", "100", "1", "100", "1e-10"];
    /// let record = BlastRecord::from_parts(&parts);
    /// ```
    pub fn from_parts(parts: &[&str]) -> Self {
        if parts.len() < 10 {
            panic!("ERROR: not enough parts to create BlastRecord -> {parts:?}");
        }

        let blast_pid = parts[1].parse::<f32>().unwrap_or_else(|_| {
            panic!("ERROR: failed to parse blast PID from parts: {:?}", parts);
        });

        let blast_e_value = parts[9].parse::<f64>().unwrap_or_else(|_| {
            panic!(
                "ERROR: failed to parse blast E-value from parts: {:?}",
                parts
            );
        });
        // INFO: if parsed to zero, but string was not "0.0", it's subnormal
        let blast_e_value = if blast_e_value == 0.0 {
            // INFO: represent it with the minimum positive value
            f64::MIN_POSITIVE // INFO: ~2.225074e-308
        } else {
            blast_e_value
        };

        let qstart = parts[5].parse::<i32>().unwrap_or_else(|_| {
            panic!(
                "ERROR: failed to parse blast offset from parts: {:?}",
                parts
            );
        });
        let qend = parts[6].parse::<i32>().unwrap_or_else(|_| {
            panic!(
                "ERROR: failed to parse blast offset from parts: {:?}",
                parts
            );
        });

        let blast_offset = parts[7].parse::<i32>().unwrap_or_else(|_| {
            panic!(
                "ERROR: failed to parse blast offset from parts: {:?}",
                parts
            )
        }) - qstart;

        let blast_alignment_len = parts[4].parse::<u32>().unwrap_or_else(|_| {
            panic!(
                "ERROR: failed to parse blast offset from parts: {:?}",
                parts
            )
        });

        let percent_aligned = (qend - qstart + 1) as f32
            / parts[2].parse::<u32>().unwrap_or_else(|_| {
                panic!(
                    "ERROR: failed to parse blast length from parts: {:?}",
                    parts
                );
            }) as f32
            * 100.0;

        Self {
            blast_pid,
            blast_e_value,
            blast_offset,
            blast_alignment_len,
            percent_aligned,
        }
    }
}

/// Read a bed file and convert it to a hashmap of genepred records
///
/// # Arguments
///
/// * `bed` - The path to the bed file
///
/// # Returns
///
/// * `HashMap<String, GenePred>` - A hashmap of genepred records
///
/// # Example
///
/// ```rust
/// let bed = get_bed(&args.bed);
/// ```
fn get_bed(bed: &PathBuf) -> HashMap<String, GenePred> {
    genepred::Reader::<Bed12>::from_mmap(bed)
        .unwrap_or_else(|e| panic!("ERROR: failed to read BED file -> {e}"))
        .filter_map(|record| {
            record.ok().map(|record| {
                (
                    from_utf8(record.name().unwrap()).unwrap().to_string(),
                    record,
                )
            })
        })
        .collect::<HashMap<String, genepred::GenePred>>()
}

/// Run orfipy on the input fasta
///
/// # Arguments
///
/// * `fasta` - The path to the input fasta file
/// * `dir` - The output directory
///
/// # Returns
///
/// * `PathBuf` - The path to the orfipy output
///
/// # Example
///
/// ```rust
/// let dir = args.outdir.join("orf");
/// std::fs::create_dir_all(&dir)
///     .unwrap_or_else(|e| panic!("ERROR: could not create directory -> {e}"));
///
/// let pep = run_orfipy(&args.fasta, &dir);
/// ```
fn run_orfipy(fasta: &Path, dir: &Path) -> PathBuf {
    let cmd = format!(
        "orfipy {} --pep {} --partial-5 --partial-3 --include-stop --strand f --min 100 --ignore-case --outdir {} --single-mode",
        fasta.display(),
        ORF_PEP,
        &dir.display()
    );

    std::process::Command::new("bash")
        .arg("-c")
        .arg(&cmd)
        .status()
        .unwrap_or_else(|e| panic!("ERROR: failed to execute orfipy command -> {e}"));

    dir.join(ORF_PEP)
}

/// Read a file into a string
///
/// # Arguments
///
/// * `file` - The path to the file
///
/// # Returns
///
/// * `Result<String, Box<dyn std::error::Error>>` - The contents of the file
///
/// # Example
///
/// ```rust
/// let pep = reader(&args.fasta);
/// ```
pub fn reader<P: AsRef<Path> + std::fmt::Debug>(
    file: P,
) -> Result<String, Box<dyn std::error::Error>> {
    let mut file = File::open(file)?;
    let mut contents = String::new();
    file.read_to_string(&mut contents)?;
    Ok(contents)
}

/// Represents a single ORF record from orfipy
///
/// # Fields
///
/// * `id` - The ID of the ORF
/// * `start` - The start position of the ORF
/// * `end` - The end position of the ORF
/// * `orf_type` - The type of the ORF
/// * `frame` - The frame of the ORF
/// * `start_codon` - The start codon of the ORF
/// * `stop_codon` - The stop codon of the ORF
/// * `strand` - The strand of the ORF
#[derive(Debug, Clone, Eq, Hash, PartialEq)]
pub struct OrfRecord {
    pub id: String,
    pub start: usize,
    pub end: usize,
    pub orf_type: OrfType,
    pub frame: i8,
    pub start_codon: String,
    pub stop_codon: String,
    pub strand: char,
    pub seq_idx: usize,
}

/// Splits a header string into its components
///
/// # Arguments
///
/// * `header` - The header string
///
/// # Returns
///
/// * `Option<(usize, usize, char)>` - The start, end, and strand of the ORF
///
/// # Example
///
/// ```rust
/// let (start, end, strand) = split_coords(coords).unwrap_or_else(|| {
///     panic!("ERROR: failed to parse coords from line: {}", line);
/// });
/// ```
pub fn split_coords(header: &str) -> Option<(usize, usize, char)> {
    let mut parts = header.split('-');

    let start = parts
        .next()
        .unwrap_or_else(|| {
            panic!("ERROR: failed to parse ORF start from header: {}", header);
        })
        .strip_prefix('[')
        .unwrap_or_else(|| {
            panic!(
                "ERROR: failed to strip prefix from ORF start from header: {}",
                header
            );
        })
        .parse::<usize>()
        .unwrap_or_else(|e| {
            panic!(
                "ERROR: failed to parse ORF start from header: {} -> {e}",
                header
            );
        });

    parts = parts
        .next()
        .unwrap_or_else(|| {
            panic!("ERROR: failed to parse ORF end from header: {}", header);
        })
        .split(']');

    let end = parts
        .next()
        .unwrap_or_else(|| {
            panic!("ERROR: failed to parse ORF end from header: {}", header);
        })
        .parse::<usize>()
        .unwrap_or_else(|e| {
            panic!(
                "ERROR: failed to parse ORF end from header: {} -> {e}",
                header
            );
        });

    let strand = parts
        .next()
        .unwrap_or_else(|| {
            panic!("ERROR: failed to parse ORF strand from header: {}", header);
        })
        .char_indices()
        .nth(1)
        .unwrap_or_else(|| {
            panic!("ERROR: failed to parse ORF strand from header: {}", header);
        })
        .1;

    Some((start, end, strand))
}

impl OrfRecord {
    /// Parses an ORF record from a string
    ///
    /// # Arguments
    ///
    /// * `line` - The string to parse
    ///
    /// # Returns
    ///
    /// * `Self` - The parsed ORF record
    ///
    /// # Example
    ///
    /// ```rust
    /// let record = OrfRecord::parse(line);
    /// ```
    pub fn parse(line: &str) -> Self {
        let mut parts = line.split(' ');

        // WARN: if headers have spaces splitting will fail!
        // INFO: >ENSMUST00000167914.2_ORF.1 [69-330](+) type:complete length:258 frame:1 start:ATG stop:TGA
        let (id, coords, orf_type, _, frame, start_codon, stop_codon) = (
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!("ERROR: failed to parse ORF ID from line: {}", line);
                })
                .to_string(),
            parts.next().unwrap_or_else(|| {
                panic!(
                    "ERROR: failed to parse ORF coords from line: {} -> {:?}",
                    line, parts
                );
            }),
            OrfType::from_str(parts.next().unwrap_or_else(|| {
                panic!("ERROR: failed to parse ORF type from line: {}", line);
            }))
            .unwrap_or_else(|e| {
                panic!("ERROR: failed to parse ORF type from line: {} -> {e}", line);
            }),
            parts.next(), // INFO: length
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!("ERROR: failed to parse ORF frame from line: {}", line);
                })
                .strip_prefix("frame:")
                .unwrap_or_else(|| {
                    panic!(
                        "ERROR: failed to strip prefix from ORF frame from line: {}",
                        line
                    );
                })
                .parse::<i8>()
                .unwrap_or_else(|e| {
                    panic!(
                        "ERROR: failed to parse ORF frame from line: {} -> {e}",
                        line
                    );
                }),
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!("ERROR: failed to parse ORF start codon from line: {}", line);
                })
                .strip_prefix("start:")
                .unwrap_or_else(|| {
                    panic!(
                        "ERROR: failed to strip prefix from ORF start codon from line: {}",
                        line
                    );
                })
                .to_string()
                .to_uppercase(),
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!("ERROR: failed to parse ORF stop codon from line: {}", line);
                })
                .strip_prefix("stop:")
                .unwrap_or_else(|| {
                    panic!(
                        "ERROR: failed to strip prefix from ORF stop codon from line: {}",
                        line
                    );
                })
                .to_string()
                .to_uppercase(),
        );

        let (start, end, strand) = split_coords(coords).unwrap_or_else(|| {
            panic!("ERROR: failed to parse coords from line: {}", line);
        });

        Self {
            id,
            start,
            end,
            orf_type,
            frame,
            start_codon,
            stop_codon,
            strand,
            seq_idx: 0, // INFO: placeholder
        }
    }
}

/// A HashHead is a tuple of (idx, seq) where idx is the index of the hash
#[derive(Debug, Clone, Eq, Hash, PartialEq)]
pub struct HashHead {
    seq: Arc<Vec<u8>>,
}

impl HashHead {
    /// Creates a new `HashHead` from an index and a sequence
    ///
    /// # Arguments
    ///
    /// * `idx` - The index of the hash
    /// * `seq` - The sequence of the hash
    ///
    /// # Returns
    ///
    /// A new `HashHead` instance
    ///
    /// # Example
    ///
    /// ```rust
    /// let hash_head = HashHead::new(0, Arc::from(vec![b'A', b'T', b'G']));
    /// ```
    pub fn new(seq: Arc<Vec<u8>>) -> Self {
        Self { seq }
    }
}

/// An enum representing the type of an ORF
#[derive(Debug, Clone, Copy, Eq, Hash, PartialEq)]
pub enum OrfType {
    Complete,
    ThreePartial,
    FivePartial,
    FivePartialNested,
    ThreePartialNested,
    CompleteNested,
}

impl std::str::FromStr for OrfType {
    type Err = Box<dyn std::error::Error>;

    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s {
            "type:complete" => Ok(OrfType::Complete),
            "type:3-prime-partial" => Ok(OrfType::ThreePartial),
            "type:5-prime-partial" => Ok(OrfType::FivePartial),
            "type:5-prime-partial-nested" => Ok(OrfType::FivePartialNested),
            "type:3-prime-partial-nested" => Ok(OrfType::ThreePartialNested),
            "type:complete-nested" => Ok(OrfType::CompleteNested),
            _ => Err(Box::new(std::io::Error::new(
                std::io::ErrorKind::InvalidData,
                format!("Invalid ORF type: {}", s),
            ))),
        }
    }
}

impl std::fmt::Display for OrfType {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            OrfType::Complete => write!(f, "CO"),
            OrfType::ThreePartial => write!(f, "TP"),
            OrfType::FivePartial => write!(f, "FP"),
            OrfType::FivePartialNested => write!(f, "FN"),
            OrfType::ThreePartialNested => write!(f, "TN"),
            OrfType::CompleteNested => write!(f, "CN"),
        }
    }
}

/// Parses a FASTA file into a hashmap of ORF records and their sequences
///
/// # Arguments
///
/// * `f` - The path to the FASTA file
///
/// # Returns
///
/// A `Result` containing a `HashMap` of `OrfRecord` and their sequences
///
/// # Example
///
/// ```rust
/// let orf_records = parse_fa("path/to/file.fa").unwrap();
/// ```
pub fn parse_fa<F: AsRef<Path>>(
    f: F,
) -> Result<HashMap<OrfRecord, Vec<u8>>, Box<dyn std::error::Error>> {
    let file = File::open(f).unwrap();
    let mmap = unsafe { Mmap::map(&file).unwrap() };
    let data = mmap.as_ref();

    let mut acc = HashMap::new();
    let mut pos = 0;

    while let Some(start) = memchr(b'>', &data[pos..]) {
        let start = pos + start;
        let end = memchr(b'>', &data[start + 1..]).map_or(data.len(), |e| start + 1 + e);
        let entry = &data[start + 1..end];
        let header_end = memchr(b'\n', entry).unwrap();
        let header = from_utf8(&entry[..header_end]).unwrap().trim();
        let record = &entry[header_end + 1..];
        let seq = record
            .iter()
            .filter(|&&b| b != b'\n' && b != b'\r') // Remove newlines and carriage returns
            .cloned()
            .collect::<Vec<u8>>();

        acc.insert(OrfRecord::parse(header), seq);
        pos = end;
    }

    Ok(acc)
}

/// Creates a new file with the given extension if it doesn't exist
///
/// # Arguments
///
/// * `file` - The path to the file
/// * `extension` - The extension of the file
///
/// # Returns
///
/// An `Option` containing a `BufWriter` for the file
///
/// # Example
///
/// ```rust
/// let writer = create_file("path/to/file.txt", "txt").unwrap();
/// ```
pub fn create_file(file: &Path, extension: &str) -> Option<BufWriter<File>> {
    let file = file.with_extension(extension);
    if !file.exists() {
        let writer = BufWriter::new(
            File::create(&file).unwrap_or_else(|e| panic!("ERROR: cannot create file -> {e}")),
        );

        Some(writer)
    } else {
        // INFO: overwrite existing fil
        dbg!("WARN: overwriting existing file -> {:?}", &file);
        std::fs::remove_file(&file).unwrap_or_else(|e| {
            panic!("ERROR: cannot remove existing file -> {e}");
        });

        Some(BufWriter::new(File::create(&file).unwrap_or_else(|e| {
            panic!("ERROR: cannot create file -> {e}")
        })))
    }
}

/// Deduplicates ORF records from a FASTA file
///
/// # Arguments
///
/// * `fasta` - The path to the FASTA file
/// * `do_nesting` - Whether to do nesting or not
/// * `min_len` - The minimum length of an ORF
/// * `min_percent` - The minimum percentage of an ORF
/// * `pattern` - The pattern to match for ORFs
///
/// # Returns
///
/// A `HashMap` of `HashHead` and their corresponding `Vec<OrfRecord>`
///
/// # Example
///
/// ```rust
/// let mapper = deduplicate("path/to/file.fa", true, 50, 0.25, &[b'M']).unwrap();
/// ```
pub fn deduplicate(
    fasta: &PathBuf,
    do_nesting: bool,
    min_len: usize,
    min_percent: f32,
    pattern: &[u8],
) -> HashMap<HashHead, Vec<OrfRecord>> {
    let seqs =
        parse_fa(fasta).unwrap_or_else(|e| panic!("ERROR: failed to parse FASTA file -> {e}"));

    let mut mapper: HashMap<HashHead, Vec<OrfRecord>> = HashMap::new(); // INFO: seq -> records
    let mut idx = 1; // INFO: stores deduplicated index of sequence -> 1-based because 0 is placeholder

    // INFO: loops through sequences and populates mapper
    for (mut header, seq) in seqs.into_iter() {
        // INFO: sequence used as key in mapper -> hhead
        let len = seq.len();
        let mut key = Vec::with_capacity(len);
        for b in &seq {
            if *b != b'\n' {
                key.push(*b);
            }
        }

        let hhead = HashHead::new(Arc::from(key));

        // INFO: if hhead exists -> header inherits seq_idx
        match mapper.entry(hhead) {
            Entry::Occupied(mut entry) => {
                // INFO: seq exists -> push header to vec
                // INFO: seq_idx is inherited from first header
                entry.get_mut().push(header.clone());
            }
            Entry::Vacant(entry) => {
                header.seq_idx = idx;
                entry.insert(vec![header.clone()]);
                idx += 1; // INFO: increment index count
            }
        }

        if do_nesting {
            __split_record(
                &header,
                &seq,
                len,
                min_len,
                min_percent,
                &mut mapper,
                pattern,
                &mut idx,
            )
        }
    }

    mapper
}

/// Splits an ORF record into nested ORF records
///
/// # Arguments
///
/// * `header` - The header of the ORF record
/// * `seq` - The sequence of the ORF record
/// * `seq_length` - The length of the sequence
/// * `min_len` - The minimum length of the ORF
/// * `min_percent` - The minimum percentage of the ORF
/// * `mapper` - The mapper to store the nested ORF records
/// * `needle` - The needle to match for the nested ORF
/// * `idx_count` - The index count to increment
///
/// # Returns
///
/// None
///
/// # Example
///
/// ```rust
/// let mapper = HashMap::new();
/// let needle = b"M";
/// let idx_count = 0;
/// __split_record(&header, &seq, seq_length, min_len, min_percent, &mut mapper, &needle, &mut idx_count);
/// ```
#[allow(clippy::too_many_arguments)]
pub fn __split_record(
    header: &OrfRecord,
    seq: &[u8],
    seq_length: usize,
    min_len: usize,
    min_percent: f32,
    mapper: &mut HashMap<HashHead, Vec<OrfRecord>>,
    needle: &[u8], // b"ATG" or b"M"
    idx_count: &mut usize,
) {
    let mut orf_count = 0;

    for (pos, &aa) in seq.iter().enumerate().skip(1) {
        // INFO: default needle is 'M' -> start signal
        if aa == needle[0] {
            let len_remaining = seq_length - pos;
            let percent = len_remaining as f32 / seq_length as f32;

            // INFO: we have found a nested ORF -> create a new record
            // WARN: updating start codons only in non-ATG cases because we are only looking for inner ATGs
            if len_remaining >= min_len && percent >= min_percent {
                orf_count += 1;

                let sub_seq = &seq[pos..];
                let mut nested_header = header.clone();

                nested_header.start += pos * 3;
                nested_header.id += format!("@{}", orf_count).as_str();

                // INFO: only updating start codon if it is not ATG
                if header.start_codon != "ATG" {
                    nested_header.start_codon = "ATG".to_string();
                }

                nested_header.orf_type = match header.orf_type {
                    OrfType::Complete => OrfType::CompleteNested,
                    OrfType::FivePartial => OrfType::FivePartialNested,
                    OrfType::ThreePartial => OrfType::ThreePartialNested,
                    _ => unreachable!(),
                };

                // INFO: subsequence is now a new key in the mapper
                let mut inner_key = Vec::with_capacity(sub_seq.len());
                for &b in sub_seq {
                    if b != b'\n' {
                        inner_key.push(b);
                    }
                }

                let hhead = HashHead::new(inner_key.into());
                match mapper.entry(hhead) {
                    Entry::Occupied(mut entry) => {
                        // INFO: seq exists -> push header to vec
                        // INFO: seq_idx is inherited from first header
                        entry.get_mut().push(nested_header.clone());
                    }
                    Entry::Vacant(entry) => {
                        nested_header.seq_idx = *idx_count;
                        entry.insert(vec![nested_header.clone()]);
                        *idx_count += 1; // INFO: increment index count
                    }
                }
            }
        }
    }
}

/// Gets the table of ORF records and their corresponding lines
///
/// # Arguments
///
/// * `mapper` - The mapper of ORF records and their sequences
/// * `bed` - The bed file
/// * `outdir` - The output directory
/// * `tai` - The path to the translationAi output
///
/// # Returns
///
/// A `HashMap` of `usize` and their corresponding `Vec<String>`
///
/// # Example
///
/// ```rust
/// let table = get_table(mapper, bed, outdir, tai).unwrap();
/// ```
pub fn get_table(
    mut mapper: HashMap<HashHead, Vec<OrfRecord>>,
    mut bed: HashMap<String, GenePred>,
    outdir: &Path,
    tai: Option<PathBuf>,
    nmd_distance: u64,
    weak_nmd_distance: i64,
    atg_distance: u64,
    big_exon_dist_to_ej: u64,
) -> HashMap<usize, Vec<String>> {
    // INFO: sequence index -> vec of constructed lines
    let mut table = HashMap::new();

    // INFO: read tai values if provided
    let mut tai = if let Some(tai) = tai {
        read_tai_table(tai)
    } else {
        HashMap::new()
    };

    let filename = &outdir.join("orf");
    let mut pep = create_file(filename, "dedup.pep")
        .unwrap_or_else(|| panic!("ERROR: could not create file {:?}", filename));

    let mut unique_tai_idx = 0;

    // INFO: iterate over mapper -> write index + mapper to records and then tai if provided
    for (head, records) in mapper.iter_mut() {
        let idx = records[0].seq_idx;

        // INFO: table holds seq idx -> vec of ORF records
        table.entry(idx).or_insert(Vec::new());

        for record in records.iter_mut() {
            unique_tai_idx += 1;

            let record_cannonical_id = record.id.split("_ORF").next().unwrap_or_else(|| {
                panic!(
                    "ERROR: failed to parse cannonical ID from header: {}",
                    record.id
                );
            });

            let gp = bed.get_mut(record_cannonical_id).unwrap_or_else(|| {
                panic!(
                    "ERROR: could not find gene prediction for ID: {}",
                    record_cannonical_id
                );
            });

            let (orf_start, orf_end) = map_absolute_cds(gp, record.start as u64, record.end as u64);
            let record_key = format!("{}:{}-{}", record_cannonical_id, orf_start, orf_end);

            // INFO: only important thing to inherit from orfipy is orf_type; strand is inherited from bed!
            if tai.contains_key(&record_key) {
                // INFO: ORF predicted by translationAi -> merge with orfipy and remove from tai
                let tai_prediction = tai.get(&record_key).unwrap_or_else(|| {
                    panic!(
                        "ERROR: could not find tai prediction for key: {}. This is a bug!",
                        record_key
                    );
                });

                if tai_prediction.relative_orf_start != record.start
                    || tai_prediction.relative_orf_end != record.end
                    || tai_prediction.start as u64 != orf_start
                    || tai_prediction.end as u64 != orf_end
                    || (record.start_codon != "NA"
                        && tai_prediction.start_codon != record.start_codon)
                    || (record.stop_codon != "NA" && tai_prediction.stop_codon != record.stop_codon)
                {
                    panic!(
                        "ERROR: tai prediction does not match orfipy prediction for key: {}. This is a bug -> {:?} vs {:?}! Key ({:?}) unlocked the following gp: {:?}",
                        record_key, tai_prediction, record, record_key, gp
                    );
                }

                let nmd_type = detect_nmd(
                    gp,
                    orf_start,
                    orf_end,
                    nmd_distance,
                    weak_nmd_distance,
                    atg_distance,
                    big_exon_dist_to_ej,
                );

                let line = format!(
                    "{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}",
                    from_utf8(&gp.chrom).unwrap(),
                    orf_start,
                    orf_end,
                    record.id,
                    gp.strand.unwrap(),
                    tai_prediction.start_score,
                    tai_prediction.stop_score,
                    record.start,
                    record.end,
                    record.start_codon,
                    tai_prediction.stop_codon, // INFO: on purpose to avoid orfipy NA stop
                    tai_prediction.inner_stops,
                    record.orf_type,
                    nmd_type
                );
                table.entry(idx).or_insert(Vec::new()).push(line);

                // INFO: remove from tai -> tai retains tai-only
                tai.remove(&record_key);
            } else {
                // INFO: ORF not predicted by translationAi -> only skipped if it is not complete
                if record.orf_type != OrfType::Complete
                    && record.orf_type != OrfType::CompleteNested
                {
                    continue;
                };

                let nmd_type = detect_nmd(
                    gp,
                    orf_start,
                    orf_end,
                    nmd_distance,
                    weak_nmd_distance,
                    atg_distance,
                    big_exon_dist_to_ej,
                );

                // INFO: orfipy complete or complete-nested -> follow tai fmt + orf_type + strand
                let line = format!(
                    "{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}",
                    from_utf8(&gp.chrom).unwrap(),
                    orf_start,
                    orf_end,
                    record.id,
                    gp.strand.unwrap(),
                    0.0, // INFO: no tai start score
                    0.0, // INFO: no tai stop score
                    record.start,
                    record.end,
                    record.start_codon,
                    record.stop_codon,
                    1, // INFO: complete or complete-nested only have 1 stop codon, otherwise would not be complete
                    record.orf_type,
                    nmd_type
                );

                table.entry(idx).or_insert(Vec::new()).push(line);
            }
        }

        let handle = table.get(&idx).unwrap_or_else(|| {
            panic!("ERROR: could not find table for head -> {:?}", head);
        });

        if handle.is_empty() {
            println!("WARN: empty table for head -> {:?}", idx);
            table.remove(&idx);
        } else {
            let hh = format!(
                ">{}\n{}",
                idx,
                from_utf8(&head.seq).unwrap_or_else(|_| {
                    panic!("ERROR: failed to convert key to UTF-8 -> {:?}", head.seq);
                })
            );
            writeln!(pep, "{}", hh).unwrap_or_else(|e| {
                panic!("ERROR: failed to write record to file -> {e} -> {:?}", hh);
            });
        }
    }

    if !tai.is_empty() {
        for (_, prediction) in tai.iter() {
            unique_tai_idx += 1;

            let rc = format!(">{}\n{}", unique_tai_idx, prediction.aa);
            writeln!(pep, "{}", rc).unwrap_or_else(|e| {
                panic!("ERROR: failed to write record to file -> {e} -> {:?}", rc);
            });

            let line = format!(
                "{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}\t{}",
                prediction.chr,
                prediction.start,
                prediction.end,
                prediction.id,
                prediction.strand,
                prediction.start_score,
                prediction.stop_score,
                prediction.relative_orf_start,
                prediction.relative_orf_end,
                prediction.start_codon,
                prediction.stop_codon,
                prediction.inner_stops,
                "UN", // INFO: unknown orf_type -> can be guessed with codons but its helpful for tai-only
            );
            table.entry(unique_tai_idx).or_insert(Vec::new()).push(line);
        }
    }

    table
}

/// An enum representing the NMD type of an ORF
pub enum NMDType {
    NN, // INFO: no_nmd
    WN, // INFO: weak_nmd
    SN, // INFO: strong_nmd
    UN, // INFO: unknown -> likely an error
}

impl std::fmt::Display for NMDType {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            NMDType::NN => write!(f, "NN"),
            NMDType::WN => write!(f, "WN"),
            NMDType::SN => write!(f, "SN"),
            NMDType::UN => write!(f, "UN"),
        }
    }
}

/// Detects the NMD type of an ORF
/// NMD types are:
/// 1. NN -> no_nmd
/// 2. WN -> weak_nmd
/// 3. SN -> strong_nmd
///
/// # Arguments
/// * `gp` - The genepred record
/// * `orf_start` - The start of the ORF
/// * `orf_end` - The end of the ORFs
/// * `nmd_distance` - The minimum 3' UTR length for a strong NMD classification.
/// * `weak_nmd_distance` - The maximum distance from the stop codon to the last exon-exon junction
///   for a weak NMD classification.
/// * `atg_distance` - The maximum CDS length for a weak NMD classification.
/// * `big_exon_dist_to_ej` - The maximum distance from the stop codon to the next splice junction
///   for a big exon test, used in weak NMD classification.
///
/// # Returns
/// A `String` representing the NMD type
///
/// # Example
/// ```rust, ignore
/// let nmd_type = detect_nmd(gp, orf_start, orf_end);
/// ```
fn detect_nmd(
    gp: &GenePred,
    orf_start: u64,
    orf_end: u64,
    nmd_distance: u64,
    weak_nmd_distance: i64,
    atg_distance: u64,
    big_exon_dist_to_ej: u64,
) -> NMDType {
    let mut cds_start = orf_start;
    let mut cds_end = orf_end;

    // INFO: noncoding transcripts + errors
    if cds_start == cds_end {
        return NMDType::UN;
    }

    let mut nmd_count: i64 = -1;
    let mut _ex_ex_junction_utr: i64 = -1;
    let mut dist_stop_to_next_sj = 0; // INFO: for big exon test
    let mut in_utr = false;
    let mut utr_len = 0;
    let mut cds_len = 0;
    let mut bp_utr_to_last_ex_ex_jct = 0;

    let mut exons = gp.exons();
    exons = match gp.strand() {
        Some(genepred::Strand::Forward) => exons,
        Some(genepred::Strand::Reverse) => {
            // INFO: we scale the coordinates and then sort them
            // INFO: this is equivalent to reversing the exons and reversing the coordinates
            let mut exons = exons
                .iter()
                .map(|(start, end)| (SCALE - end, SCALE - start))
                .collect::<Vec<(u64, u64)>>();

            exons.sort_by(|a, b| a.0.cmp(&b.0));

            cds_end = SCALE - orf_start;
            cds_start = SCALE - orf_end;

            exons
        }
        None | Some(genepred::Strand::Unknown) => {
            panic!(
                "ERROR: failed to parse strand from genepred record -> {:?}",
                gp
            )
        }
    };

    for (i, exon) in exons.iter().enumerate() {
        let exon_start = exon.0;
        let exon_end = exon.1;

        // INFO: Count EEJs in 3'UTR
        if exon_end >= cds_end {
            _ex_ex_junction_utr += 1;

            // INFO: first exon containing stop codon
            if dist_stop_to_next_sj == 0 {
                dist_stop_to_next_sj = exon_end - cds_end;
            }

            if !in_utr {
                utr_len += exon_end - cds_end;
                in_utr = true;
            } else {
                utr_len += exon_end - exon_start;
            }

            if utr_len >= nmd_distance {
                nmd_count += 1;
            }

            // If last exon, compute bpUTRtoLastEEJ
            if i == exons.len() - 1 {
                bp_utr_to_last_ex_ex_jct = utr_len as i64 - (exon_end as i64 - exon_start as i64);
            }
        }

        // INFO: CDS length accumulation
        if exon_end < cds_start || exon_start > cds_end {
            continue; // INFO: skip pure UTR exons
        }

        // INFO: first coding exon
        if exon_end >= cds_start && cds_start >= exon_start {
            if exon_end >= cds_end {
                cds_len += cds_end - cds_start;
            } else {
                cds_len += exon_end - cds_start;
            }
        }
        // INFO: internal coding exon
        else if exon_start > cds_start && exon_end < cds_end {
            cds_len += exon_end - exon_start;
        }
        // INFO: last coding exon
        else if exon_start > cds_start && exon_end >= cds_end {
            cds_len += cds_end - exon_start;
        }
    }

    // INFO: final classification -> tag [NN: no_nmd, SN: strong_nmd, WN: weak_nmd]
    if nmd_count == 0 || nmd_count == -1 {
        NMDType::NN
    } else {
        if bp_utr_to_last_ex_ex_jct <= weak_nmd_distance
            || dist_stop_to_next_sj >= big_exon_dist_to_ej
            || cds_len <= atg_distance
        {
            NMDType::WN
        } else {
            NMDType::SN
        }
    }
}

/// Reads the translationAi output and converts it to a hashmap of `TaiRecord`s
///
/// # Arguments
///
/// * `tai` - The path to the translationAi output
///
/// # Returns
///
/// A `HashMap` of `String` and their corresponding `TaiRecord`
///
/// # Example
///
/// ```rust
/// let tai = read_tai_table(tai).unwrap();
/// ```
fn read_tai_table(tai: PathBuf) -> HashMap<String, TaiRecord> {
    let predictions = reader(tai)
        .unwrap_or_else(|e| panic!("ERROR: failed to read blast predictions file -> {e}"));

    let mut accumulator = HashMap::new();

    // INFO: chr17 26054102 26060105 ENSMUST00000176751.2.p3 - 0.011934959 0.9905367 242 2126 ATG TAG 8
    for line in predictions.lines() {
        let record = TaiRecord::parse(line);
        let cannonical_id =
            record.id.split(".p").next().unwrap_or_else(|| {
                panic!("ERRROR: failed to parse cannonical ID from line: {}", line)
            }); // INFO: ENSMUST00000176751.2.p3 -> ENSMUST00000176751.2
        let key = format!("{}:{}-{}", cannonical_id, record.start, record.end); // INFO: ENSMUST00000176751.2:26054102-26060105
        accumulator.insert(key, record);
    }

    accumulator
}

/// A struct representing a translationAi record
#[derive(Debug, Clone)]
pub struct TaiRecord {
    pub chr: String,
    pub start: usize,
    pub end: usize,
    pub id: String,
    pub strand: char,
    pub start_score: f32,
    pub stop_score: f32,
    pub relative_orf_start: usize,
    pub relative_orf_end: usize,
    pub start_codon: String,
    pub stop_codon: String,
    pub inner_stops: usize,
    pub aa: String,
}

impl TaiRecord {
    pub fn parse(line: &str) -> Self {
        let mut parts = line.split('\t');

        let (
            chr,
            start,
            end,
            id,
            strand,
            start_score,
            stop_score,
            relative_orf_start,
            relative_orf_end,
            start_codon,
            stop_codon,
            inner_stops,
            aa,
        ) = (
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!("ERROR: failed to parse tai chr from line: {}", line);
                })
                .to_string(),
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!("ERROR: failed to parse tai start from line: {}", line);
                })
                .parse::<usize>()
                .unwrap_or_else(|e| {
                    panic!(
                        "ERROR: failed to parse tai start from line: {} -> {e}",
                        line
                    );
                }),
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!("ERROR: failed to parse tai end from line: {}", line);
                })
                .parse::<usize>()
                .unwrap_or_else(|e| {
                    panic!("ERROR: failed to parse tai end from line: {} -> {e}", line);
                }),
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!("ERROR: failed to parse tai id from line: {}", line);
                })
                .to_string(),
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!("ERROR: failed to parse tai strand from line: {}", line);
                })
                .chars()
                .next()
                .unwrap_or_else(|| {
                    panic!("ERROR: failed to parse tai strand from line: {}", line);
                }),
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!("ERROR: failed to parse tai start score from line: {}", line);
                })
                .parse::<f32>()
                .unwrap_or_else(|e| {
                    panic!(
                        "ERROR: failed to parse tai start score from line: {} -> {e}",
                        line
                    );
                }),
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!("ERROR: failed to parse tai stop score from line: {}", line);
                })
                .parse::<f32>()
                .unwrap_or_else(|e| {
                    panic!(
                        "ERROR: failed to parse tai stop score from line: {} -> {e}",
                        line
                    );
                }),
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!(
                        "ERROR: failed to parse relative orf start from line: {}",
                        line
                    );
                })
                .parse::<usize>()
                .unwrap_or_else(|e| {
                    panic!(
                        "ERROR: failed to parse relative orf start from line: {} -> {e}",
                        line
                    );
                }),
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!(
                        "ERROR: failed to parse relative orf end from line: {}",
                        line
                    );
                })
                .parse::<usize>()
                .unwrap_or_else(|e| {
                    panic!(
                        "ERROR: failed to parse relative orf end from line: {} -> {e}",
                        line
                    );
                }),
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!("ERROR: failed to parse start codon from line: {}", line);
                })
                .to_string()
                .to_uppercase(),
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!("ERROR: failed to parse stop codon from line: {}", line);
                })
                .to_string()
                .to_uppercase(),
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!("ERROR: failed to parse inner stops from line: {}", line);
                })
                .parse::<usize>()
                .unwrap_or_else(|e| {
                    panic!(
                        "ERROR: failed to parse inner stops from line: {} -> {e}",
                        line
                    );
                }),
            parts
                .next()
                .unwrap_or_else(|| {
                    panic!("ERROR: failed to parse aa from line: {}", line);
                })
                .to_string(),
        );

        Self {
            chr,
            start,
            end,
            id,
            strand,
            start_score,
            stop_score,
            relative_orf_start,
            relative_orf_end,
            start_codon,
            stop_codon,
            inner_stops,
            aa,
        }
    }
}
